{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read & Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"Cats&Dogs_Dataset.csv\")\n",
    "\n",
    "# splitting the dataset into training and testing 80:20\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# only rows in image column that contain string with cat\n",
    "df_cat = df[df[\"image\"].str.contains(\"cat\")]\n",
    "# only rows in image column that contain string with dog\n",
    "df_dog = df[df[\"image\"].str.contains(\"dog\")]\n",
    "\n",
    "# train test split\n",
    "train_cat, test_cat = train_test_split(df_cat, test_size=0.2, random_state=1)\n",
    "train_dog, test_dog = train_test_split(df_dog, test_size=0.2, random_state=1)\n",
    "\n",
    "#take only 10 rows from each dataset to train the model\n",
    "train_cat = train_cat.head(2)\n",
    "train_dog = train_dog.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Clip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip \n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model once at the beginning\n",
    "# model, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\") \n",
    "# model.eval() # move model to GPU and set it to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "# torch.save(model, 'model.pt')\n",
    "\n",
    "#load the model\n",
    "model = torch.load('model.pt')\n",
    "model , preprocess = clip.load(\"ViT-B/32\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import skimage as sk #image processing library\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image #image processing library\n",
    "\n",
    "#images destination\n",
    "descriptions = {\n",
    "\"C\",\n",
    "\"D\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = []\n",
    "processed_images = []\n",
    "descriptions = [\"C\", \"D\"]  # Change set to list\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "# preprocess the images\n",
    "for i in range(0, 4):\n",
    "    var = ''\n",
    "    if i % 2 == 0:\n",
    "        var = 'cat.' + str(i)\n",
    "    else:\n",
    "        var = 'dog.' + str(i)\n",
    "    # original image\n",
    "    original_image = Image.open('./Cats&Dogs_Pics/' + var + '.jpg')\n",
    "    original_images.append(original_image)\n",
    "    # processed image\n",
    "    processed_image = preprocess(original_image)\n",
    "    processed_images.append(processed_image)\n",
    "    # description\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow(original_image)\n",
    "    if i % 2 == 0:\n",
    "        plt.title(descriptions[0])\n",
    "    else:\n",
    "        plt.title(descriptions[1])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Building Features\n",
    "image_input_train_cat = torch.stack([preprocess(Image.open('./Cats&Dogs_Pics/' + img_path)) for img_path in train_cat['image']]) #preprocess the images and stack them into a tensor\n",
    "image_input_train_dog = torch.stack([preprocess(Image.open('./Cats&Dogs_Pics/' + img_path)) for img_path in train_dog['image']]) #preprocess the images and stack them into a tensor\n",
    "\n",
    "text_tokens_train_cat = clip.tokenize(descriptions[0] * len(train_cat)) #tokenize the text and stack them into a tensor\n",
    "text_tokens_train_dog = clip.tokenize(descriptions[1] * len(train_dog)) #tokenize the text and stack them into a tensor\n",
    "\n",
    "text_features_train_cat = model.encode_text(text_tokens_train_cat).float() #encode the text and convert to float\n",
    "text_features_train_dog = model.encode_text(text_tokens_train_dog).float() #encode the text and convert to float\n",
    "\n",
    "# Combine image and text features\n",
    "with torch.no_grad(): #disable gradient calculation to speed up computation and reduce memory consumption\n",
    "    image_features_train_cat = model.encode_image(image_input_train_cat).float()\n",
    "    image_features_train_dog = model.encode_image(image_input_train_dog).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_visualize_combined_cosine_similarity(image_features_cat, text_features_cat, image_features_dog, text_features_dog, descriptions, original_images):\n",
    "    # Normalize the features\n",
    "    image_features_cat /= image_features_cat.norm(dim=-1, keepdim=True)\n",
    "    text_features_cat /= text_features_cat.norm(dim=-1, keepdim=True)\n",
    "    image_features_dog /= image_features_dog.norm(dim=-1, keepdim=True)\n",
    "    text_features_dog /= text_features_dog.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Calculate cosine similarity for both cats and dogs\n",
    "    similarity_matrix_cat = text_features_cat.detach().numpy() @ image_features_cat.detach().numpy().T\n",
    "    similarity_matrix_dog = text_features_dog.detach().numpy() @ image_features_dog.detach().numpy().T\n",
    "\n",
    "    # Create a combined visualization\n",
    "    count_cat = len(descriptions[0])\n",
    "    count_dog = len(descriptions[1])\n",
    "\n",
    "    plt.figure(figsize=(20, 14))\n",
    "\n",
    "    # Plot Cat Images\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for i, image in enumerate(original_images[:count_cat]):\n",
    "        plt.imshow(image, extent=(i - 0.5, i + 0.5, -0.6, 0.6), origin=\"upper\")  # Adjust extent and origin\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.title(\"Original Images - Cats\", size=20)\n",
    "\n",
    "    for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "        plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "    # Plot Cosine Similarity Matrix for Cats\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.imshow(similarity_matrix_cat, vmin=0.1, vmax=0.3, cmap='viridis', origin=\"upper\")  # Adjust origin\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.title(\"Cosine Similarity Matrix - Cats\", size=20)\n",
    "\n",
    "    for x in range(similarity_matrix_cat.shape[1]):\n",
    "        for y in range(similarity_matrix_cat.shape[0]):\n",
    "            plt.text(x, y, f\"{similarity_matrix_cat[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
    "\n",
    "    for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "        plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "    # Plot Dog Images\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for i, image in enumerate(original_images[count_cat:count_cat + count_dog]):\n",
    "        plt.imshow(image, extent=(i - 0.5, i + 0.5, -0.6, 0.6), origin=\"upper\")  # Adjust extent and origin\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.title(\"Original Images - Dogs\", size=20)\n",
    "\n",
    "    for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "        plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "    # Plot Cosine Similarity Matrix for Dogs\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(similarity_matrix_dog, vmin=0.1, vmax=0.3, cmap='viridis', origin=\"upper\")  # Adjust origin\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.title(\"Cosine Similarity Matrix - Dogs\", size=20)\n",
    "\n",
    "    for x in range(similarity_matrix_dog.shape[1]):\n",
    "        for y in range(similarity_matrix_dog.shape[0]):\n",
    "            plt.text(x, y, f\"{similarity_matrix_dog[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
    "\n",
    "    for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "        plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize Combined Cosine Similarity Matrix for Cats and Dogs\n",
    "calculate_and_visualize_combined_cosine_similarity(image_features_train_cat, text_features_train_cat, image_features_train_dog, text_features_train_dog, descriptions, original_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Evaluation (You need to modify this based on your specific labels)\n",
    "predictions = (similarity_cat > similarity_dog).astype(int)\n",
    "ground_truth = np.concatenate([np.zeros(len(similarity_cat)), np.ones(len(similarity_dog))])\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "conf_matrix = confusion_matrix(ground_truth, predictions)\n",
    "accuracy = accuracy_score(ground_truth, predictions)\n",
    "precision = precision_score(ground_truth, predictions)\n",
    "recall = recall_score(ground_truth, predictions)\n",
    "f1 = f1_score(ground_truth, predictions)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "To generate text descriptions for images using the CLIP model, you would typically follow these steps:\n",
    "\n",
    "1. Load the image and preprocess it to the format expected by the CLIP model.\n",
    "2. Use `model.encode_image(image:Tensor)` to convert the image into a feature vector.\n",
    "3. Generate a set of candidate descriptions (these could be completely random, or they could be based on some prior knowledge).\n",
    "4. Use `clip.tokenize` to convert these descriptions into the format expected by the CLIP model.\n",
    "5. Use `model.encode_text` to convert these tokenized descriptions into feature vectors.\n",
    "6. Compare the image feature vector to the description feature vectors to find the best match. The description corresponding to the closest-matching feature vector is the model's generated description of the image.\n",
    "\n",
    "Here's a Python code snippet that demonstrates these steps:\n",
    "\n",
    "\n",
    "This code assumes that you have the CLIP model and the necessary libraries (torch, torchvision, PIL) installed. If not, you can install them with pip:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from PIL import Image\n",
    "# from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "# from clip import clip, tokenize\n",
    "\n",
    "# # Load the image\n",
    "# image_path = \"path_to_your_image.jpg\"\n",
    "# image = Image.open(image_path)\n",
    "\n",
    "# # Preprocess the image\n",
    "# preprocess = Compose([\n",
    "#     Resize(256), \n",
    "#     CenterCrop(224), \n",
    "#     ToTensor(), \n",
    "#     Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "# ])\n",
    "# image = preprocess(image)\n",
    "# image = image.unsqueeze(0)  # add batch dimension\n",
    "\n",
    "# # Load the CLIP model\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model, transform = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# # Encode the image to get a feature vector\n",
    "# image_features = model.encode_image(image.to(device))\n",
    "\n",
    "# # Generate some candidate descriptions\n",
    "# descriptions = [\"A cat on a sofa\", \"A dog in a park\", \"A group of people at the beach\"]\n",
    "\n",
    "# # Tokenize and encode the descriptions to get feature vectors\n",
    "# description_tokens = clip.tokenize(descriptions).to(device)\n",
    "# description_features = model.encode_text(description_tokens)\n",
    "\n",
    "# # Compare the image feature vector to the description feature vectors\n",
    "# # The dot product between two vectors is a measure of how similar they are\n",
    "# similarities = (image_features @ description_features.T).softmax(dim=-1)\n",
    "\n",
    "# # Get the index of the most similar description\n",
    "# best_match_index = similarities.argmax(dim=-1).item()\n",
    "\n",
    "# # Print the best-matching description\n",
    "# print(f\"Generated description: {descriptions[best_match_index]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
